# Adversarial Attacks

This repository contains a notebook that focuses on training a Convolutional Neural Network (CNN) using the CIFAR-10 dataset. The CNN architecture is defined in the resnet.py file. Additionally, the notebook covers the implementation of targeted and untargeted Fast Gradient Sign Method (FGSM) attacks, as well as the $l_{∞}$-Projected Gradient Descent (PGD) attack. The performance of the trained model will be evaluated using adversarial data generated by these attacks.

## Attacks
### Targeted FGSM Attack:
This attack aims to misclassify input images by perturbing them in a direction that maximizes the loss of the target class. By calculating the gradient of the loss function with respect to the input, small perturbations are added to the image pixels to shift the classification decision toward the desired target class.

### Untargeted FGSM Attack:
Similar to the targeted attack, the untargeted FGSM attack also perturbs input images. However, in this case, the objective is to maximize the loss of the true class, leading to misclassification into any incorrect class. The gradient of the loss function is computed with respect to the input, and perturbations are added to maximize the loss and cause misclassification.

### $l_{∞}$-PGD Attack:
The $l_{∞}$-PGD attack is a more powerful and iterative attack method. It performs multiple iterations of perturbations, each time adjusting the input image within a small $l_{∞}$-norm bound. This iterative process progressively maximizes the loss and produces adversarial examples that are challenging for the model to classify correctly.

By implementing these attacks and evaluating the model's performance on the adversarial data, we gain insights into the model's robustness and its vulnerability to different types of attacks.

For more detailed information and code implementation, please refer to the notebook files in this repository.
